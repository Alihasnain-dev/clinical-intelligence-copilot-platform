{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.12/dist-packages (12.27.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from azure-storage-blob) (1.36.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from azure-storage-blob) (43.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-storage-blob) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from azure-storage-blob) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage-blob python-dotenv pandas scikit-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SAS_TOKEN not found in environment variables or manual override.\n",
      "Please paste your SAS Token in the input box that appears at the TOP of the screen (VS Code) or below (Colab).\n",
      "✅ Using Storage Account: clinicaldatalake25\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('../.env')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# If the .env file isn't loading, PASTE YOUR SAS TOKEN BELOW between the quotes:\n",
    "MANUAL_SAS_TOKEN = \"\"\n",
    "\n",
    "STORAGE_ACCOUNT_NAME = os.getenv(\"STORAGE_ACCOUNT_NAME\", \"clinicaldatalake25\")\n",
    "\n",
    "# Logic to determine which token to use\n",
    "SAS_TOKEN = MANUAL_SAS_TOKEN if MANUAL_SAS_TOKEN else os.getenv(\"SAS_TOKEN\", \"\")\n",
    "\n",
    "if not SAS_TOKEN:\n",
    "    print(\"⚠️ SAS_TOKEN not found in environment variables or manual override.\")\n",
    "    print(\"Please paste your SAS Token in the input box that appears at the TOP of the screen (VS Code) or below (Colab).\")\n",
    "    try:\n",
    "        SAS_TOKEN = input(\"Enter SAS Token: \").strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not SAS_TOKEN:\n",
    "    raise ValueError(\"SAS_TOKEN is required to proceed. Please paste it in the variable 'MANUAL_SAS_TOKEN' above and run this cell again.\")\n",
    "\n",
    "print(f\"✅ Using Storage Account: {STORAGE_ACCOUNT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️ Connecting to Azure to download dataset...\n",
      "✅ Successfully downloaded and loaded 'PatientNoShowKaggleMay2016.csv'.\n",
      "Columns sanitized for processing.\n"
     ]
    }
   ],
   "source": [
    "# This notebook now operates completely on cloud data.\n",
    "# Ensure you have run 'scripts/upload_csv_to_azure.py' locally first.\n",
    "\n",
    "try:\n",
    "    print(\"☁️ Connecting to Azure to download dataset...\")\n",
    "    account_url = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient(account_url=account_url, credential=SAS_TOKEN)\n",
    "    \n",
    "    DATA_CONTAINER = \"data-source\"\n",
    "    DATA_BLOB = \"PatientNoShowKaggleMay2016.csv\"\n",
    "    \n",
    "    blob_client = blob_service_client.get_blob_client(container=DATA_CONTAINER, blob=DATA_BLOB)\n",
    "    downloader = blob_client.download_blob()\n",
    "    \n",
    "    # Load data directly into pandas from the downloaded stream\n",
    "    df = pd.read_csv(io.BytesIO(downloader.readall()))\n",
    "    \n",
    "    print(f\"✅ Successfully downloaded and loaded '{DATA_BLOB}'.\")\n",
    "    \n",
    "    # Sanitize column names to match the rest of the script\n",
    "    df.columns = [c.lower().replace('-', '') for c in df.columns]\n",
    "    print(\"Columns sanitized for processing.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Failed to download data from Azure: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noshow'] = df['noshow'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df['gender'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "df['scheduledday'] = pd.to_datetime(df['scheduledday'])\n",
    "df['appointmentday'] = pd.to_datetime(df['appointmentday'])\n",
    "\n",
    "df['scheduled_year'] = df['scheduledday'].dt.year\n",
    "df['scheduled_month'] = df['scheduledday'].dt.month\n",
    "df['scheduled_day'] = df['scheduledday'].dt.day\n",
    "df['scheduled_weekday'] = df['scheduledday'].dt.dayofweek\n",
    "\n",
    "# Calculate Lead Days (Time between scheduling and appointment)\n",
    "df['lead_days'] = (df['appointmentday'].dt.normalize() - df['scheduledday'].dt.normalize()).dt.days\n",
    "# Ensure no negative lead days\n",
    "df['lead_days'] = df['lead_days'].apply(lambda x: max(0, x))\n",
    "\n",
    "categorical_features = ['neighbourhood']\n",
    "for col in categorical_features:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-and-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same-Day Records: 38568\n",
      "Future Records: 71959\n",
      "\n",
      "--- Training Same-Day Model ---\n",
      "[LightGBM] [Info] Number of positive: 1468, number of negative: 29386\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 30854, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047579 -> initscore=-2.996617\n",
      "[LightGBM] [Info] Start training from score -2.996617\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's binary_logloss: 0.157822\n",
      "\n",
      "--- Training Future Model ---\n",
      "[LightGBM] [Info] Number of positive: 16431, number of negative: 41136\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 359\n",
      "[LightGBM] [Info] Number of data points in the train set: 57567, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.285424 -> initscore=-0.917714\n",
      "[LightGBM] [Info] Start training from score -0.917714\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's binary_logloss: 0.583472\n"
     ]
    }
   ],
   "source": [
    "# --- SPLIT STRATEGY ---\n",
    "# We will train two separate models:\n",
    "# 1. Same-Day Model (Lead Days == 0)\n",
    "# 2. Future Model (Lead Days > 0)\n",
    "# This prevents the 'SMS Received' feature from acting as a proxy for 'Future Appointment' in the Future Model.\n",
    "\n",
    "df_same_day = df[df['lead_days'] == 0]\n",
    "df_future = df[df['lead_days'] > 0]\n",
    "\n",
    "print(f\"Same-Day Records: {len(df_same_day)}\")\n",
    "print(f\"Future Records: {len(df_future)}\")\n",
    "\n",
    "features = ['gender', 'age', 'neighbourhood', 'scholarship', 'hipertension', 'diabetes', 'alcoholism', 'handcap', 'sms_received', 'scheduled_year', 'scheduled_month', 'scheduled_day', 'scheduled_weekday', 'lead_days']\n",
    "target = 'noshow'\n",
    "\n",
    "def train_model(dataframe, model_name):\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    X = dataframe[features]\n",
    "    y = dataframe[target]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=50,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "    return gbm\n",
    "\n",
    "model_same_day = train_model(df_same_day, \"Same-Day Model\")\n",
    "model_future = train_model(df_future, \"Future Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-upload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved locally as 'no_show_model.pkl'\n",
      "\n",
      "Uploading model artifacts to Azure Blob Storage...\n",
      "✅ SUCCESS: Dual-Model artifacts uploaded to container 'ml-models' as 'ops/no_show_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save both models in a single dictionary\n",
    "model_artifacts = {\n",
    "    \"same_day_model\": model_same_day,\n",
    "    \"future_model\": model_future\n",
    "}\n",
    "\n",
    "with open('no_show_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "print(\"Models saved locally as 'no_show_model.pkl'\")\n",
    "\n",
    "# --- Upload to Azure Blob Storage ---\n",
    "print(\"\\nUploading model artifacts to Azure Blob Storage...\")\n",
    "\n",
    "if not SAS_TOKEN:\n",
    "    print(\"⚠️  WARNING: SAS_TOKEN not found. Skipping upload.\")\n",
    "else:\n",
    "    try:\n",
    "        account_url = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "        blob_service_client = BlobServiceClient(account_url=account_url, credential=SAS_TOKEN)\n",
    "        \n",
    "        CONTAINER_NAME = \"ml-models\"\n",
    "        BLOB_NAME = \"ops/no_show_model.pkl\"\n",
    "        \n",
    "        # Create container if needed\n",
    "        try:\n",
    "            blob_service_client.create_container(CONTAINER_NAME)\n",
    "        except Exception:\n",
    "            pass # Container likely exists\n",
    "\n",
    "        blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=BLOB_NAME)\n",
    "        \n",
    "        with open(\"no_show_model.pkl\", \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "        print(f\"✅ SUCCESS: Dual-Model artifacts uploaded to container '{CONTAINER_NAME}' as '{BLOB_NAME}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Failed to upload to Azure: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
